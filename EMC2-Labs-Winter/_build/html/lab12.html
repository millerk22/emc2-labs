<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Lab 12: One-dimensional Optimization and Gradient Descent &#8212; Math 495R EMC2 Python Labs</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="_static/material.css?v=79c92029" />
    <script src="_static/documentation_options.js?v=6fefd858"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lab 13: Exploring Sequences and Limit Points" href="lab13.html" />
    <link rel="prev" title="Lab 11: Root Finding and Newton’s Method" href="lab11.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#lab12" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="index.html" title="Math 495R EMC2 Python Labs"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Winter EMC2 Labs</span>
          <span class="md-header-nav__topic"> Lab 12: One-dimensional Optimization and Gradient Descent </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = ""versions.json"",
        target_loc = "../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="index.html" class="md-tabs__link">Math 495R EMC2 Python Labs</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="index.html" title="Math 495R EMC2 Python Labs" class="md-nav__button md-logo">
      
        <img src="_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="index.html"
       title="Math 495R EMC2 Python Labs">Winter EMC2 Labs</a>
  </label>
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
    
      <a href="SQLlab.html" class="md-nav__link">Lab 0: An Introduction to SQL</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab01.html" class="md-nav__link">Lab 1: Introduction to Plotting</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab02.html" class="md-nav__link">Lab 2: Introduction to Plotting and Sequences</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab03.html" class="md-nav__link">Lab 3: Introduction to Space Curves</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab04.html" class="md-nav__link">Lab 4: Searching in a Sorted List</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab05.html" class="md-nav__link">Lab 5: Planes, Trains and Automobiles</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab06.html" class="md-nav__link">Lab 6: Sorting and Complexity of Algorithms</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab07.html" class="md-nav__link">Lab 7: Introduction to Series</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab08.html" class="md-nav__link">Lab 8: Plotting Surfaces in 3-D</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab09.html" class="md-nav__link">Lab 9: Symbolic Python and Partial Differentiation</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab10.html" class="md-nav__link">Lab 10: A Very Basic Program</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab11.html" class="md-nav__link">Lab 11: Root Finding and Newton’s Method</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    <label class="md-nav__link md-nav__link--active" for="__toc"> Lab 12: One-dimensional Optimization and Gradient Descent </label>
    
      <a href="#" class="md-nav__link md-nav__link--active">Lab 12: One-dimensional Optimization and Gradient Descent</a>
      
        
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#lab12--page-root" class="md-nav__link">Lab 12: One-dimensional Optimization and Gradient Descent</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#newtons-method-for-optimization" class="md-nav__link">Newton’s Method for optimization</a>
        </li>
        <li class="md-nav__item"><a href="#task-1" class="md-nav__link">Task 1</a>
        </li>
        <li class="md-nav__item"><a href="#task-2" class="md-nav__link">Task 2</a>
        </li>
        <li class="md-nav__item"><a href="#descent-methods" class="md-nav__link">Descent Methods</a>
        </li>
        <li class="md-nav__item"><a href="#the-method-of-steepest-descent" class="md-nav__link">The Method of Steepest Descent</a>
        </li>
        <li class="md-nav__item"><a href="#task-3" class="md-nav__link">Task 3</a>
        </li>
        <li class="md-nav__item"><a href="#task-4" class="md-nav__link">Task 4</a>
        </li>
        <li class="md-nav__item"><a href="#task-5" class="md-nav__link">Task 5</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="_sources/lab12.rst.txt">Show Source</a> </li>

  </ul>
</nav>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab13.html" class="md-nav__link">Lab 13: Exploring Sequences and Limit Points</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab14.html" class="md-nav__link">Lab 14: Riemann Sums</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab15.html" class="md-nav__link">Lab 15: Monte Carlo Integration</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab16.html" class="md-nav__link">Lab 16: Geometry in Higher Dimensions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab17.html" class="md-nav__link">Lab 17: Rearrangements of Conditionally Convergent Series</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab18.html" class="md-nav__link">Lab 18: K-Means Clustering</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab19.html" class="md-nav__link">Lab 19: Plotting Vector Valued Functions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab20.html" class="md-nav__link">Lab 20: Approximate Integration</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab21.html" class="md-nav__link">Lab 21: Iterating Functions</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab22.html" class="md-nav__link">Lab 22: Fractals with Turtle</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab23.html" class="md-nav__link">Lab 23: Finding Areas of Arbitrary Polygons</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="lab24.html" class="md-nav__link">Lab 24: Basins of Attraction</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#lab12--page-root" class="md-nav__link">Lab 12: One-dimensional Optimization and Gradient Descent</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#newtons-method-for-optimization" class="md-nav__link">Newton’s Method for optimization</a>
        </li>
        <li class="md-nav__item"><a href="#task-1" class="md-nav__link">Task 1</a>
        </li>
        <li class="md-nav__item"><a href="#task-2" class="md-nav__link">Task 2</a>
        </li>
        <li class="md-nav__item"><a href="#descent-methods" class="md-nav__link">Descent Methods</a>
        </li>
        <li class="md-nav__item"><a href="#the-method-of-steepest-descent" class="md-nav__link">The Method of Steepest Descent</a>
        </li>
        <li class="md-nav__item"><a href="#task-3" class="md-nav__link">Task 3</a>
        </li>
        <li class="md-nav__item"><a href="#task-4" class="md-nav__link">Task 4</a>
        </li>
        <li class="md-nav__item"><a href="#task-5" class="md-nav__link">Task 5</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="_sources/lab12.rst.txt">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="lab-12-one-dimensional-optimization-and-gradient-descent">
<h1 id="lab12--page-root">Lab 12: One-dimensional Optimization and Gradient Descent<a class="headerlink" href="#lab12--page-root" title="Link to this heading">¶</a></h1>
<p>Most mathematical optimization problems involve estimating the minimizer(s) of a scalar-valued function. Many algorithms for optimizing functions with a high-dimensional domain depend on routines for optimizing functions of a single variable. There are many techniques for optimization in one dimension, each with varying degrees of precision and speed. In your lab for 341, you used Newton’s method to find the roots of an equation. We will adapt this method to find optimal points (i.e., maxima and minima) of a one-dimensional equation. After doing this we will consider the problem of gradient descent.</p>
<section id="newtons-method-for-optimization">
<h2 id="newtons-method-for-optimization">Newton’s Method for optimization<a class="headerlink" href="#newtons-method-for-optimization" title="Link to this heading">¶</a></h2>
<p>One might not expect that Newton’s method can also be used for optimization. Recall that the first-order necessary conditions from elementary calculus state that if <span class="math notranslate nohighlight">\(f\)</span> is differentiable, then its derivative evaluates to zero at each of its local minima and maxima. Therefore using Newton’s method to find the zeros of <span class="math notranslate nohighlight">\(f'\)</span> is a way to identify potential minima or maxima of <span class="math notranslate nohighlight">\(f\)</span>. Specifically, starting with an initial guess <span class="math notranslate nohighlight">\(x_0\)</span>, set</p>
<div class="math notranslate nohighlight">
\[x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}\]</div>
<p>and iterate until <span class="math notranslate nohighlight">\(|x_k - x_{k-1}|\)</span> is satisfactorily small. Note that this procedure does not use the actual function <span class="math notranslate nohighlight">\(f\)</span> at all, but it requires many evaluations of its first and second derivatives. As a result, Newton’s method converges in few iterations, but it can be computationally expensive.</p>
<p>Newton’s method for optimization works well to locate minima when <span class="math notranslate nohighlight">\(f''(x) &gt; 0\)</span> on the entire domain. However, it may fail to converge to a minimizer if <span class="math notranslate nohighlight">\(f''(x) \leq 0\)</span> for some portion of the domain. If <span class="math notranslate nohighlight">\(f\)</span> is not unimodal, the initial guess <span class="math notranslate nohighlight">\(x_0\)</span> must be sufficiently close to a local minimizer in order to converge.</p>
</section>
<section id="task-1">
<h2 id="task-1">Task 1<a class="headerlink" href="#task-1" title="Link to this heading">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(f : \mathbb R \to \mathbb R\)</span>.
Adapt your code for Newton’s method from <a class="reference internal" href="lab11.html"><span class="doc">Lab 11: Root Finding and Newton’s Method</span></a> to write a function, <code class="docutils literal notranslate"><span class="pre">newton(df,</span> <span class="pre">d2f,</span> <span class="pre">x0,</span> <span class="pre">tol,</span> <span class="pre">maxiter)</span></code>, that accepts the first and second derivatives of a function, <code class="docutils literal notranslate"><span class="pre">df</span></code> and <code class="docutils literal notranslate"><span class="pre">d2f</span></code>, a starting point, <code class="docutils literal notranslate"><span class="pre">x0</span></code> (defaulting to <code class="docutils literal notranslate"><span class="pre">0</span></code>), a stopping tolerance, <code class="docutils literal notranslate"><span class="pre">tol</span></code> (defaulting to <code class="docutils literal notranslate"><span class="pre">1e-8</span></code>), and a maximum number of iterations, <code class="docutils literal notranslate"><span class="pre">maxiter</span></code> (defaulting to <code class="docutils literal notranslate"><span class="pre">100</span></code>). Implement Newton’s method using the formula above to locate a local optimizer. Return the approximate optimizer, whether or not the algorithm converged, and the number of iterations computed.</p>
</section>
<section id="task-2">
<h2 id="task-2">Task 2<a class="headerlink" href="#task-2" title="Link to this heading">¶</a></h2>
<p>Test your function from Task 1 by minimizing <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">+</span> <span class="pre">sin(5</span> <span class="pre">*</span> <span class="pre">x)</span></code> with various initial guesses, tolerances, and iteration constraints. Compare your results to <code class="docutils literal notranslate"><span class="pre">opt.newton()</span></code>, which implements the root-finding version of Newton’s method.</p>
</section>
<section id="descent-methods">
<h2 id="descent-methods">Descent Methods<a class="headerlink" href="#descent-methods" title="Link to this heading">¶</a></h2>
<p>Consider now a function <span class="math notranslate nohighlight">\(f : \mathbb R^n \to \mathbb R\)</span>. <em>Descent methods</em>, also called <em>line search methods</em>, are optimization algorithms that create a convergent sequence <span class="math notranslate nohighlight">\((x_k)\)</span> by the following rule.</p>
<div class="math notranslate nohighlight">
\[{\mathbf x}_{k+1} = {\mathbf x}_k + \alpha_k {\mathbf p}_k\]</div>
<p>Here <span class="math notranslate nohighlight">\(\alpha_k\)</span> is called the <em>step size</em> and <span class="math notranslate nohighlight">\(\mathbf p_k\)</span> is called the <em>search direction</em>.
The choice of <span class="math notranslate nohighlight">\(\mathbf p_k\)</span> is usually what distinguishes an algorithm; in the one-dimensional case <span class="math notranslate nohighlight">\(n = 1\)</span>,</p>
<div class="math notranslate nohighlight">
\[p_k = \frac{f'(x_k)}{f''(x_k)}\]</div>
<p>results in Newton’s method.</p>
<p>To be effective, a descent method must also use a good step size <span class="math notranslate nohighlight">\(\alpha_k\)</span>. If <span class="math notranslate nohighlight">\(\alpha_k\)</span> is too large, the method may repeatedly overstep the minimum, as shown in the figure below; if <span class="math notranslate nohighlight">\(\alpha_k\)</span> is too small, the method may converge extremely slowly.</p>
<a class="reference internal image-reference" href="_images/overstep.png"><img alt="_images/overstep.png" class="align-center" src="_images/overstep.png" style="width: 45%;"/></a>
<p>Given a search direction <span class="math notranslate nohighlight">\(\mathbf p_k\)</span>, the best step size <span class="math notranslate nohighlight">\(\alpha_k\)</span> minimizes the function</p>
<div class="math notranslate nohighlight">
\[\phi_k(\alpha) = f({\mathbf x}_k + \alpha {\mathbf p}_k)\]</div>
<p>Since f is scalar-valued, <span class="math notranslate nohighlight">\(\phi_k : \mathbb R \to \mathbb R\)</span>, so Newton’s method (or any other 1-D optimization method) can be used to minimize <span class="math notranslate nohighlight">\(\phi_k\)</span>.</p>
</section>
<section id="the-method-of-steepest-descent">
<h2 id="the-method-of-steepest-descent">The Method of Steepest Descent<a class="headerlink" href="#the-method-of-steepest-descent" title="Link to this heading">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(f:\mathbb R^n\to\mathbb R\)</span> with first derivative <span class="math notranslate nohighlight">\(Df:\mathbb R^n\to \mathbb R^n\)</span>.
The following iterative technique is a common template for methods that aim to compute a local minimizer <span class="math notranslate nohighlight">\(\mathbf x^*\)</span> of <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-steepest">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-steepest" title="Link to this equation">¶</a></span>\[\mathbf x_{k+1} = \mathbf x_k + \alpha_k \mathbf p_k\]</div>
<p>Here <span class="math notranslate nohighlight">\(\mathbf x_k\)</span> is the k-th approximation to <span class="math notranslate nohighlight">\(\mathbf x^*\)</span>, <span class="math notranslate nohighlight">\(\alpha_k\)</span> is the <em>step size</em>, and <span class="math notranslate nohighlight">\(\mathbf p_k\)</span> is the <em>search direction</em>. Newton’s method and its relatives follow this pattern, but they require the calculation (or approximation) of the inverse Hessian matrix <span class="math notranslate nohighlight">\(Df^2(\mathbf x_k)^{-1}\)</span> at each step. The following idea is a simpler and less computationally intensive approach than Newton and quasi-Newton methods.</p>
<p>The derivative <span class="math notranslate nohighlight">\(D f(x)^T\)</span> (often called the <em>gradient</em> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>, sometimes notated <span class="math notranslate nohighlight">\(\nabla f(x)\)</span>) is a vector that points in the direction of greatest <strong>increase</strong> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. It follows that the negative derivative <span class="math notranslate nohighlight">\(-D f(x)^T\)</span> points in the direction of steepest <strong>decrease</strong> at <span class="math notranslate nohighlight">\(\mathbf x\)</span>. The <em>method of steepest descent</em> chooses the search direction <span class="math notranslate nohighlight">\(\mathbf p_k = -D f(\mathbf x_k)^T\)</span> at each step of <a class="reference internal" href="#equation-eq-steepest">(1)</a>, resulting in the following algorithm.</p>
<div class="math notranslate nohighlight" id="equation-eq-steepest-2">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-steepest-2" title="Link to this equation">¶</a></span>\[\mathbf x_{k+1} = \mathbf x_k - \alpha_k D f(\mathbf x_k)^T\]</div>
<p>Setting <span class="math notranslate nohighlight">\(\alpha_k = 1\)</span> for each <span class="math notranslate nohighlight">\(k\)</span> is often sufficient for Newton and quasi-Newton methods.
However, a constant choice for the step size in <a class="reference internal" href="#equation-eq-steepest-2">(2)</a> can result in oscillating approximations or even cause the sequence <span class="math notranslate nohighlight">\((\mathbf x_k)_{k=1}^\infty\)</span> to travel away from the minimizer <span class="math notranslate nohighlight">\(\mathbf x^*\)</span>.
To avoid this problem, the step size <span class="math notranslate nohighlight">\(\alpha_k\)</span> can be chosen in a few ways.</p>
<ul>
<li><p>Start with <span class="math notranslate nohighlight">\(\alpha_k = 1\)</span>, then set <span class="math notranslate nohighlight">\(\alpha_k = \frac{1}{2}\alpha_k\)</span> until <span class="math notranslate nohighlight">\(f(\mathbf x_k - \alpha_k D f(\mathbf x_k)^T) &lt; f(\mathbf x_k)\)</span>, terminating the iteration if <span class="math notranslate nohighlight">\(\alpha_k\)</span> gets too small. This guarantees that the method actually descends at each step and that <span class="math notranslate nohighlight">\(\alpha_k\)</span> satisfies the Armijo rule, without endangering convergence.</p></li>
<li><p>At each step, solve the following one-dimensional optimization problem.</p>
<div class="math notranslate nohighlight">
\[\alpha_k = \mathrm{argmin}_{\alpha} f(\mathbf x_k - \alpha D f (\mathbf x_k)^T)\]</div>
<p>Using this choice is called <em>exact steepest descent</em>. This option is more expensive per iteration than the above strategy, but it results in fewer iterations before convergence.</p>
</li>
</ul>
</section>
<section id="task-3">
<h2 id="task-3">Task 3<a class="headerlink" href="#task-3" title="Link to this heading">¶</a></h2>
<p>Write a function, <code class="docutils literal notranslate"><span class="pre">grad_descent_const(df,</span> <span class="pre">x0,</span> <span class="pre">a,</span> <span class="pre">tol,</span> <span class="pre">maxiter)</span></code> that accepts the derivative of an objective function, <code class="docutils literal notranslate"><span class="pre">df</span></code>, an initial guess, <code class="docutils literal notranslate"><span class="pre">x0</span></code>, a constant step size, <code class="docutils literal notranslate"><span class="pre">a</span></code>, a convergence tolerance, <code class="docutils literal notranslate"><span class="pre">tol</span></code>, defaulting to <code class="docutils literal notranslate"><span class="pre">1e-8</span></code>, and a maximum number of iterations, <code class="docutils literal notranslate"><span class="pre">maxiter</span></code>, defaulting to <code class="docutils literal notranslate"><span class="pre">100</span></code>, and computes the minimizer via the constant method of steepest descent (gradient descent with constant step size). Return the approximate minimizer, whether or not the algorithm converged, and the number of iterations computed.</p>
<p>Why does satisfying the convergence condition not guarantee that we converged sufficiently close to a minimizer for this version of gradient descent?</p>
</section>
<section id="task-4">
<h2 id="task-4">Task 4<a class="headerlink" href="#task-4" title="Link to this heading">¶</a></h2>
<p>Adapt your code from the previous exercise to write a function, <code class="docutils literal notranslate"><span class="pre">grad_descent_exact(f,</span> <span class="pre">df,</span> <span class="pre">x0,</span> <span class="pre">tol,</span> <span class="pre">maxiter)</span></code>, that takes as input a differentiable function, <code class="docutils literal notranslate"><span class="pre">f</span></code>, its derivative, <code class="docutils literal notranslate"><span class="pre">df</span></code>, an initial guess, <code class="docutils literal notranslate"><span class="pre">x0</span></code>, an allowed error tolerance, <code class="docutils literal notranslate"><span class="pre">tol</span></code>, and a maximum number of iterations, <code class="docutils literal notranslate"><span class="pre">maxiter</span></code>, and uses exact gradient descent to find a minimizer for <code class="docutils literal notranslate"><span class="pre">f</span></code>. Return the minimizer, whether the algorithm converged within the error tolerance, and the number of iterations computed.</p>
<p>Specifically, compute <code class="docutils literal notranslate"><span class="pre">a</span></code> for each step of the algorithm by finding the minimizer of <code class="docutils literal notranslate"><span class="pre">f(x</span> <span class="pre">-</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">df(x))</span></code> for a fixed <code class="docutils literal notranslate"><span class="pre">x</span></code> instead of accepting it as an argument.</p>
</section>
<section id="task-5">
<h2 id="task-5">Task 5<a class="headerlink" href="#task-5" title="Link to this heading">¶</a></h2>
<p>The convergence of an algorithm is the function, <code class="docutils literal notranslate"><span class="pre">L(tol)</span></code>, (often simplified to the largest order) that describes how the number of iterations to converge within the error tolerance as that tolerance gets closer to zero. Newton’s method for optimization is a quadratic method and exact gradient descent is a linear method. Optimization methods tend to work best when the function we are optimizing on is convex. This is because convex functions have a unique minimizer.</p>
<p>The Rosenbrock function, <code class="docutils literal notranslate"><span class="pre">f</span> <span class="pre">=</span> <span class="pre">lambda</span> <span class="pre">x,</span> <span class="pre">y:</span> <span class="pre">(a</span> <span class="pre">-</span> <span class="pre">x)</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">+</span> <span class="pre">b</span> <span class="pre">*</span> <span class="pre">(y</span> <span class="pre">-</span> <span class="pre">x</span> <span class="pre">**</span> <span class="pre">2)</span> <span class="pre">**</span> <span class="pre">2</span></code> is non-convex for all values of <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code> except <code class="docutils literal notranslate"><span class="pre">a,</span> <span class="pre">b</span> <span class="pre">=</span> <span class="pre">0,</span> <span class="pre">0</span></code>. Classically, we set <code class="docutils literal notranslate"><span class="pre">a,</span> <span class="pre">b</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">100</span></code>. As such, it takes much longer for gradient descent to converge for this function than most convex functions.</p>
<p>Write a function, <code class="docutils literal notranslate"><span class="pre">plot_convergence(a,</span> <span class="pre">b)</span></code>, that takes as input the constants for the Rosenbrock function, <code class="docutils literal notranslate"><span class="pre">a,</span> <span class="pre">b</span></code>, and performs gradient descent on the corresponding version of the Rosenbrock function for many values of <code class="docutils literal notranslate"><span class="pre">tol</span></code>, recording the number of iterations needed to converge (making sure that the algorithm converged), using <code class="docutils literal notranslate"><span class="pre">x0</span> <span class="pre">=</span> <span class="pre">np.zeros(2)</span></code>, and then creates a plot of the number of iterations needed to converge, <code class="docutils literal notranslate"><span class="pre">n</span></code>, as a function of the required error tolerance, <code class="docutils literal notranslate"><span class="pre">tol</span></code>, with <code class="docutils literal notranslate"><span class="pre">tol</span></code> measured on a logarithmic scale (use <code class="docutils literal notranslate"><span class="pre">plt.semilogx</span></code> instead of <code class="docutils literal notranslate"><span class="pre">plt.plot</span></code>). Make sure to label your plot and axes.</p>
<p>Notice that your graph ought to look approximately linear i.e. like a step function that is bounded above by a linear function for the given axes.</p>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="lab11.html" title="Lab 11: Root Finding and Newton’s Method"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> "Previous" </span> Lab 11: Root Finding and Newton’s Method </span>
              </div>
            </a>
          
          
            <a href="lab13.html" title="Lab 13: Exploring Sequences and Limit Points"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> "Next" </span> Lab 13: Exploring Sequences and Limit Points </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, Nick Andersen.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>